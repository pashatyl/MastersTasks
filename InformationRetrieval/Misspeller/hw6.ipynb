{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 6\n",
    "\n",
    "В данном домашнем задании Вам предстоит реализовать автоматическое исправление опечаток в запросах пользователей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Датасет\n",
    "Для оценки качества алгоритма исправления опечаток, Вам предоставляется файл `queries.tsv.gz`. В каждой строке файла записаны два запроса – исходный и исправленный. Для простоты, оба запроса будут иметь одинаковое количество слов и отличаться незначительно. Зачастую исходный и исправленный запрос совпадают, что означает что исправлять такой запрос не требуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Generator, Callable\n",
    "\n",
    "Query = str\n",
    "Sentence = str\n",
    "Filename = str\n",
    "Word = str\n",
    "Queries = List[Tuple[Query, Query]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lake compo\u001b[32mu\u001b[0mnd the park\n",
      "traditional c\u001b[31mh\u001b[0m\u001b[32ml\u001b[0mothes\n",
      "\u001b[32mc\u001b[0m\u001b[32ma\u001b[0m\u001b[32mp\u001b[0m\u001b[32mt\u001b[0m\u001b[32ma\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32m \u001b[0mjack sparrow\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import difflib\n",
    "\n",
    "def diff_queries(original: Query, fixed: Query) -> Query:\n",
    "    result = ''\n",
    "    for pos, d in enumerate(difflib.ndiff(original, fixed)):\n",
    "        if d[0] == '+':\n",
    "            result += colored(d[2], 'green')\n",
    "        elif d[0] == '-':\n",
    "            result += colored(d[2], 'red')\n",
    "        else:\n",
    "            result += d[2]\n",
    "    return result\n",
    "\n",
    "print(diff_queries(\"lake compond the park\", \"lake compound the park\"))\n",
    "print(diff_queries(\"traditional chothes\", \"traditional clothes\"))\n",
    "print(diff_queries(\"jack sparrow\", \"captain jack sparrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 102436 queries\n",
      "\n",
      "emb\u001b[31me\u001b[0m\u001b[32ma\u001b[0mr\u001b[31mi\u001b[0m\u001b[32mr\u001b[0m\u001b[32ma\u001b[0mssing red carpet moments\n",
      "grants for rural areas flo\u001b[32mr\u001b[0mi\u001b[31mr\u001b[0mda\n",
      "the home \u001b[31mh\u001b[0m\u001b[32md\u001b[0mepot merchandising\n",
      "delaware motorcycle inspectio\u001b[32mn\u001b[0m requirements\n",
      "highland park hospital gastric b\u001b[31mi\u001b[0m\u001b[32my\u001b[0mpass surgery\n",
      "grand the\u001b[31mi\u001b[0mft auto\n",
      "windward community college\n",
      "my credit reports\n",
      "st\u001b[32mr\u001b[0mack intermediate school\n",
      "mongol empire political system\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "def load_queries(fn: Filename) -> Queries:\n",
    "    result = []\n",
    "    with gzip.open(fn, 'rt', encoding='utf8') as inp:\n",
    "        for line in inp:\n",
    "            original, fixed = line.rstrip('\\n').split('\\t')\n",
    "            result.append((original, fixed))\n",
    "    return result\n",
    "\n",
    "queries = load_queries(\"queries.tsv.gz\")\n",
    "print(f'Loaded {len(queries)} queries\\n')\n",
    "for original, fixed in queries[10:20]:\n",
    "    print(diff_queries(original, fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_sample = [\n",
    "    (\"grand theift auto\", \"grand theft auto\"),\n",
    "    (\"belarus longitude and latitdue\", \"belarus longitude and latitude\"),\n",
    "    (\"search for poeoms\", \"search for poems\"),\n",
    "    (\"large guacolmoi dip restaurtant price\", \"large guacamole dip restaurant price\"),\n",
    "    (\"texas chainsaw mascurer\", \"texas chainsaw massacre\"),\n",
    "    (\"royal trump subtitle\", \"royal tramp subtitle\"),\n",
    "    (\"florida fiberglass polls\", \"florida fiberglass pools\"),\n",
    "    (\"how to make a calender\", \"how to make a calendar\"),\n",
    "    (\"university of south caroline\", \"university of south carolina\"),\n",
    "    (\"maureen mcdonald in virginia\", \"maureen mcdonnell in virginia\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для составления словаря и обучения языковых моделей Вам предоставляется небольшой корпус текста, неслучайная выборка из большой английской википедии в файле `train.bz2`. Этот файл содержит примерно 5 млн строк или 80 млн слов. Каждая строка – одно предложение без знаков препинания.\n",
    "Использование других словарей и корпусов запрещено."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 237.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gol neshin\n",
      "mitochondrial dna depletion syndrome mds or mdds is any of a group of autosomal recessive disorders that cause a significant drop in mitochondrial dna in affected tissues\n",
      "following the relegation of sc freiburg in 2005 he was on the verge of signing for metalurg donetsk but instead he accepted a contract with vfl wolfsburg\n",
      "the first issue for geometers is what kind of geometry is adequate for a novel situation\n",
      "cedar grove was formerly a stage and freight stop\n",
      "regular bus service runs from bhubaneswar to niali which is away\n",
      "later they were also known for the cream wafer biscuits\n",
      "strabomantis cornutus\n",
      "gtk+ scene graph kit gsk was initially released as part of gtk+ 3.90 in march 2017 and is meant for gtk-based applications that wish to replace clutter for their ui\n",
      "the match took place on 10 april 1906 at the hipódromo madrid\n",
      "the brothers came from fresno california\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_huge_corpus(fn: Filename) -> Generator[Sentence, None, None]:\n",
    "    with bz2.open(fn, 'rt', encoding='utf8') as inp:\n",
    "        for line in tqdm(inp):\n",
    "            yield line.rstrip('\\n')\n",
    "\n",
    "for li, line in enumerate(read_huge_corpus(\"./train.bz2\")):\n",
    "    print(line)\n",
    "    if li == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Поиск близких слов\n",
    "Требуется научится быстро находить список из сотни слов, которые незначительно отличаются от заданного слова.\n",
    "\n",
    "Не стоит перебирать все слова словаря – займёт слишком много времени.\n",
    "\n",
    "Для ускорения перебора предлагается создать триграммный индекс – для каждой буквенной триграммы храним список слов, в которых она есть. Тогда для поиска похожих на данное слово найдем слова большим количеством совпадающих триграмм. \n",
    "\n",
    "Совет 1: стоит сделать отельный индекс для каждой длинны слова и использовать только те индексы, в которых лежат слова близкие по длине к исходному.\n",
    "\n",
    "Совет 2: для выделения триграмм стоит обрамить слово спецсимволом, чтобы триграммы на концах слова отличались от оных в середине.\n",
    "\n",
    "Любые другие алгоритмы, улучшающие качество за разумное время (хождение по бору с ошибками, перебор ошибок) – не возбраняются.\n",
    "\n",
    "Не побрезгуйте кешировать результат работы этого алгоритма, чтобы дальнейшая работа протекала быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "if os.path.exists(\"all_words\"):\n",
    "    all_words = pickle.load(open(\"all_words\", 'rb'))\n",
    "else:\n",
    "    all_words = defaultdict(int)\n",
    "    train = read_huge_corpus(\"./train.bz2\")\n",
    "    for ind, text in enumerate(train):\n",
    "        for word in text.split(\" \"):\n",
    "            all_words[word] += 1\n",
    "    all_words = {i: all_words[i] for i in all_words if all_words[i] > 5}\n",
    "    pickle.dump(all_words, open(\"all_words\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228997/228997 [00:02<00:00, 79931.41it/s]\n"
     ]
    }
   ],
   "source": [
    "trigram_index = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "def trigrams(word, m=3):\n",
    "    padded = \"$$\" + word + \"$$\"\n",
    "    return [padded[i:i+m] for i in range(len(padded)-m+1)]\n",
    "\n",
    "for word in tqdm(all_words.keys()):\n",
    "    for trigram in trigrams(word):\n",
    "        trigram_index[len(word)][trigram].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102436/102436 [00:00<00:00, 361629.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('chothes', 'clothes'),\n",
       " ('cataloges', 'catalogs'),\n",
       " ('compond', 'compound'),\n",
       " ('barns', 'barnes'),\n",
       " ('emberissing', 'embarrassing'),\n",
       " ('floirda', 'florida'),\n",
       " ('hepot', 'depot'),\n",
       " ('inspectio', 'inspection'),\n",
       " ('bipass', 'bypass'),\n",
       " ('theift', 'theft')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_fix = []#[(o, f) for o, f in zip(query[0].split(\" \"), query[1].split(\" \")) if o != f]\n",
    "for query in tqdm(queries):\n",
    "    words_to_fix += [(o, f) for o, f in zip(query[0].split(\" \"), query[1].split(\" \")) if o != f]\n",
    "words_to_fix[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chothes - ok\n",
      "  clothes\n",
      "  choices\n",
      "  crathes\n",
      "  chooses\n",
      "  chores\n",
      "\n",
      "cataloges - ok\n",
      "  catalogues\n",
      "  cataloged\n",
      "  catalogus\n",
      "  catalogs\n",
      "  catalyses\n",
      "\n",
      "compond - ok\n",
      "  compound\n",
      "  composed\n",
      "  component\n",
      "  commend\n",
      "  compose\n",
      "\n",
      "barns - ok\n",
      "  barns\n",
      "  bairns\n",
      "  barnes\n",
      "  barnas\n",
      "  barons\n",
      "\n",
      "emberissing - ok\n",
      "  embarrassing\n",
      "  embossing\n",
      "  remembering\n",
      "  embezzling\n",
      "  embellishing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from functools import lru_cache \n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def find_similar_words(word: Word, len_gap=2, N=1000) -> List[Word]:\n",
    "    similar = []\n",
    "    for trigram in trigrams(word):\n",
    "        for word_len in range(len(word) - len_gap, len(word) + len_gap + 1):\n",
    "            if trigram in trigram_index[word_len]:\n",
    "                similar += trigram_index[word_len][trigram]\n",
    "    similar = Counter(similar)\n",
    "    # TODO: replace with partial sort\n",
    "    return sorted(\n",
    "        list(similar.keys()), \n",
    "        key=lambda x:(similar[x] / len(word), -abs(len(x) - len(word))),\n",
    "        reverse=True)[:N]\n",
    "\n",
    "for original, fixed in words_to_fix[:5]:\n",
    "    similar = find_similar_words(original)\n",
    "    print(original, '- ok' if fixed in similar else '- fail')\n",
    "    for word in similar[:5]:\n",
    "        print(' ', word)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оценить качество полученного алгоритма, используйте запросы из `queries.tsv.gz`. Отберите только отличающиеся слова в исправленном и исходном запросах. Проверьте, что для слова в исходном запросе, исправленное слово будет в списке ближайших выданном вашим алгоритмом. Если это выполняется для всех или почти всех пар – успех. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53495 words to fix\n",
      "c\u001b[31mh\u001b[0m\u001b[32ml\u001b[0mothes\n",
      "catalog\u001b[31me\u001b[0ms\n",
      "compo\u001b[32mu\u001b[0mnd\n",
      "barn\u001b[32me\u001b[0ms\n",
      "emb\u001b[31me\u001b[0m\u001b[32ma\u001b[0mr\u001b[31mi\u001b[0m\u001b[32mr\u001b[0m\u001b[32ma\u001b[0mssing\n",
      "flo\u001b[32mr\u001b[0mi\u001b[31mr\u001b[0mda\n",
      "\u001b[31mh\u001b[0m\u001b[32md\u001b[0mepot\n",
      "inspectio\u001b[32mn\u001b[0m\n",
      "b\u001b[31mi\u001b[0m\u001b[32my\u001b[0mpass\n",
      "the\u001b[31mi\u001b[0mft\n"
     ]
    }
   ],
   "source": [
    "def extract_different_words(queries: Queries) -> List[Tuple[Word, Word]]:\n",
    "    words_to_fix = []\n",
    "    for original, fixed in queries:\n",
    "        if original != fixed:\n",
    "            for word_orig, word_fixed in zip(original.split(), fixed.split()):\n",
    "                if word_orig != word_fixed:\n",
    "                    words_to_fix.append((word_orig, word_fixed))\n",
    "    return words_to_fix\n",
    "                    \n",
    "words_to_fix = extract_different_words(queries)\n",
    "print(f'Found {len(words_to_fix)} words to fix')\n",
    "for original, fixed in words_to_fix[:10]:\n",
    "    print(diff_queries(original, fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrong: 115 - 6.86%:   3%|▎         | 1675/53495 [01:18<40:27, 21.35it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-9e3365b67365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Wrong: {wrong} - {wrong/total*100:0.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcheck_find_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_to_fix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_similar_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-156-9e3365b67365>\u001b[0m in \u001b[0;36mcheck_find_similar_words\u001b[0;34m(words_to_fix, find_similar_words)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mwrong\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Wrong: {wrong} - {wrong/total*100:0.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcheck_find_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_to_fix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_similar_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mset_description\u001b[0;34m(self, desc, refresh)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_description_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_to_similar = {}\n",
    "\n",
    "def check_find_similar_words(words_to_fix: List[Tuple[Word, Word]], \n",
    "                             find_similar_words: Callable[[Word], List[Word]]):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(words_to_fix)\n",
    "    for word_orig, word_fixed in progress:\n",
    "        similar = find_similar_words(word_orig)\n",
    "        if word_fixed not in similar:\n",
    "            wrong += 1\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')\n",
    "        \n",
    "check_find_similar_words(words_to_fix, find_similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Языковая модель\n",
    "Языковая модель – модель, которая по тексту оценивает вероятность того, что он мог появиться в языке. \n",
    "\n",
    "Постройте простую n-грамную языковую модель с использованием корпуса текстов `train.bz2`. Для этого рассчитайте количество вхождений каждой n-граммы в корпус текста. Если взять n=2, то размера оперативной памяти вашего компьютера должно будет хватить.\n",
    "\n",
    "Воспользуйтесь каким-нибудь методом сглаживания, чтобы не получать нулевую вероятность для неизвестных n-грамм. Также, чтобы вероятности слов, которых нет в словаре, были отличны от нуля, можно примешать побуквенную m-граммную модель.\n",
    "\n",
    "Совет N: если количество оперативной памяти прижмёт, можно хранить строки в виде байт – один раскодированный символ занимает больше памяти чем один байт, при этом для английского текста почти всегда один символ кодируется одним байтом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 3\n",
    "\n",
    "def word_ngram(words, n):\n",
    "    return [\" \".join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "if os.path.exists(\"n_grams\"):\n",
    "    n_grams = pickle.load(open(\"n_grams\", 'rb'))\n",
    "    m_grams = pickle.load(open(\"m_grams\", 'rb'))\n",
    "else:\n",
    "    n_grams = defaultdict(int)\n",
    "    m_grams = defaultdict(int)\n",
    "    train = read_huge_corpus(\"./train.bz2\")\n",
    "    for text in train:\n",
    "        words = text.split(\" \")\n",
    "        for n_gram in word_ngram(words, n):\n",
    "            n_grams[n_gram] += 1\n",
    "            \n",
    "        for word in text.split(\" \"):\n",
    "            for m_gram in trigrams(word, m):\n",
    "                m_grams[m_gram] += 1\n",
    "\n",
    "    pickle.dump(n_grams, open(\"n_grams\", 'wb'))\n",
    "    pickle.dump(m_grams, open(\"m_grams\", 'wb'))\n",
    "\n",
    "total_n_grams = sum(n_grams[n_gram] for n_gram in n_grams)\n",
    "total_m_grams = sum(m_grams[m_gram] for m_gram in m_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok]                          grand theift auto  0.44  <   1.00 grand theft auto\n",
      "[ok]             belarus longitude and latitdue  0.75  <   1.00 belarus longitude and latitude\n",
      "[ok]                          search for poeoms  0.67  <   1.00 search for poems\n",
      "[ok]      large guacolmoi dip restaurtant price  0.41  <   0.80 large guacamole dip restaurant price\n",
      "[ok]                    texas chainsaw mascurer  0.67  <   1.00 texas chainsaw massacre\n",
      "[ok]                       royal trump subtitle  1.00  <   1.00 royal tramp subtitle\n",
      "[fail]                 florida fiberglass polls  1.00  >=  1.00 florida fiberglass pools\n",
      "[ok]                     how to make a calender  0.80  <   1.00 how to make a calendar\n",
      "[ok]               university of south caroline  1.00  <   1.00 university of south carolina\n",
      "[fail]             maureen mcdonald in virginia  1.00  >=  1.00 maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "total_words = sum(all_words[word] for word in all_words)\n",
    "\n",
    "def get_probability(query: Query) -> float:\n",
    "    probability = 1\n",
    "    words = query.split(\" \")\n",
    "    for n_gram in word_ngram(words, n):\n",
    "        if n_gram not in n_grams:\n",
    "            probability *= 1 - n / len(n_grams)\n",
    "            for word in n_gram.split(\" \"):\n",
    "                if word not in all_words:\n",
    "                    probability *= 1 - 1 / len(words)\n",
    "                    for m_gram in trigrams(word, m):\n",
    "                        if m_gram not in m_grams:\n",
    "                            probability = 1 - all_words[word] / len(m_grams)\n",
    "    return probability\n",
    "\n",
    "for original, fixed in queries_sample:\n",
    "    p_original = get_probability(original)\n",
    "    p_fixed = get_probability(fixed)\n",
    "    verdict = '[ok]  ' if p_fixed > p_original else '[fail]'\n",
    "    sign = '< ' if p_fixed > p_original else '>='\n",
    "    print(f'{verdict} {original:>40s} {p_original:5.2f}  {sign} {p_fixed:5.2f} {fixed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оценить качество полученной модели, используйте запросы из `queries.tsv.gz`. Сравните вероятность, которую выдает ваша модель для исходных и исправленных запросов. Хорошая модель выдаёт исправленному запросу большую вероятность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrong: 141 - 8.38%:   3%|▎         | 3346/102436 [00:06<03:06, 531.55it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'miami3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-37170eaf9454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Wrong: {wrong} - {wrong/total*100:0.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mcheck_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-174-37170eaf9454>\u001b[0m in \u001b[0;36mcheck_language_model\u001b[0;34m(queries, get_probability, debug)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfixed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mp_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mp_fixed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp_fixed\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mp_original\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-92eede621401>\u001b[0m in \u001b[0;36mget_probability\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mm_gram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mm_gram\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm_grams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                             \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_grams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'miami3'"
     ]
    }
   ],
   "source": [
    "def check_language_model(queries: Queries, get_probability: Callable[[Query], float], debug: bool):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(queries)\n",
    "    debug_output = 0\n",
    "    for original, fixed in progress:\n",
    "        if original == fixed:\n",
    "            continue\n",
    "        p_original = get_probability(original)\n",
    "        p_fixed = get_probability(fixed)\n",
    "        if p_fixed <= p_original:\n",
    "            wrong += 1\n",
    "            if debug:\n",
    "                print(original, p_original)\n",
    "                print(fixed, p_fixed)\n",
    "                print()\n",
    "                debug_output += 1\n",
    "                if debug_output == 10:\n",
    "                    break\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')\n",
    "        \n",
    "check_language_model(queries, get_probability, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Советую сохранить полученную модель на диск – а случае чего, чтение статистик с диска, может быть быстрее расчёта оных с нуля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Модель ошибок\n",
    "Модель ошибок – модель которая по исходному и исправленному запросу оценивает вероятность того, что такая ошибка могла быть допущена.\n",
    "\n",
    "Рассчитайте простую модель ошибок на основе расстояния Дамерау-Левенштейна, то есть модифицированного Левенштейна, который считает перестановку соседних букв за одну ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grand theift auto |  0.94 | grand theft auto\n",
      "          belarus longitude and latitdue |  0.97 | belarus longitude and latitude\n",
      "                       search for poeoms |  0.94 | search for poems\n",
      "   large guacolmoi dip restaurtant price |  0.86 | large guacamole dip restaurant price\n",
      "                 texas chainsaw mascurer |  0.83 | texas chainsaw massacre\n",
      "                    royal trump subtitle |  0.95 | royal tramp subtitle\n",
      "                florida fiberglass polls |  0.96 | florida fiberglass pools\n",
      "                  how to make a calender |  0.95 | how to make a calendar\n",
      "            university of south caroline |  0.96 | university of south carolina\n",
      "            maureen mcdonald in virginia |  0.90 | maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "from fastDamerauLevenshtein import damerauLevenshtein\n",
    "\n",
    "def get_error_probability(original: Query, fixed: Query) -> float:\n",
    "    return damerauLevenshtein(original, fixed, similarity=True)\n",
    "\n",
    "for original, fixed in queries_sample:\n",
    "    p_error = get_error_probability(original, fixed)\n",
    "    print(f'{original:>40s} | {p_error:5.2f} | {fixed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Олтугеза\n",
    "Объедините результат работы предыдущих пунктов в единый алгоритм исправления опечатки для запроса.\n",
    "\n",
    "Примерный план:\n",
    "1.\tДля слов запроса генерируем список ближайших слов-кандидатов (для всех, даже словарных слов).\n",
    "2.\tСобираем список кандидатов-запросов (эвристически, чтобы не сделать экспоненциальное время выполнения)\n",
    "3.\tДля каждого кандидата считаем итоговый объединенный score на основе языковой модели и модели ошибок для данного кандидата (не обязательно сумма или произведение, можно объединение любой сложности).\n",
    "4.\tВыдаём гипотезу с наибольшим score.\n",
    "5.\t???\n",
    "6.\tProfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok]                           grand theft auto == grand theft auto\n",
      "[ok]             belarus longitude and latitude == belarus longitude and latitude\n",
      "[ok]                           search for poems == search for poems\n",
      "[fail]       large giacomo dip restaurant price != large guacamole dip restaurant price\n",
      "[fail]                    texas chainsaw maurer != texas chainsaw massacre\n",
      "[fail]                     royal trump subtitle != royal tramp subtitle\n",
      "[fail]                 florida fiberglass polls != florida fiberglass pools\n",
      "[ok]                     how to make a calendar == how to make a calendar\n",
      "[fail]             university of south caroline != university of south carolina\n",
      "[fail]             maureen mcdonald in virginia != maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from itertools import product\n",
    "\n",
    "def correct(query: Query, words_top=2) -> Query:\n",
    "    queries = []\n",
    "    similar = {}\n",
    "    for word in query.split(\" \"):\n",
    "        similar[word] = sorted(\n",
    "            find_similar_words(word),\n",
    "            key=lambda x: -get_error_probability(word, x)\n",
    "        )[:words_top]\n",
    "    similar_queries = sorted(\n",
    "        [\" \".join(pr) for pr in product(*[similar[word] for word in query.split(\" \")])],\n",
    "        key=lambda x: get_probability(x),\n",
    "        reverse=True\n",
    "    )\n",
    "    return similar_queries[0]\n",
    "\n",
    "for original, fixed in queries_sample:\n",
    "    predict = correct(original)\n",
    "    verdict = '[ok]  ' if predict == fixed else '[fail]'\n",
    "    sign = '==' if predict == fixed else '!='\n",
    "    print(f'{verdict} {predict:>40s} {sign} {fixed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговое качество меряем на примерах из `queries.tsv.gz`.\n",
    "\n",
    "Для отладки проблем с качеством имеет смысл научится понимать на каком этапе теряется правильная гипотеза для каждого примера. Например, если правильное исправление есть в списке кандидатов (п. 2), но не выбирается как лучшая – стоит крутить языковую модель, модель ошибок и их объединение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrong: 2722 - 25.25%:  11%|█         | 11162/102436 [10:12<1:23:30, 18.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-642e68a7305e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Wrong: {wrong} - {wrong/total*100:0.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mcheck_corrector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-642e68a7305e>\u001b[0m in \u001b[0;36mcheck_corrector\u001b[0;34m(queries, correct, debug)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfixed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mwrong\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-575e8792f9bd>\u001b[0m in \u001b[0;36mcorrect\u001b[0;34m(query, words_top)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         similar[word] = sorted(\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mfind_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mget_error_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         )[:words_top]\n",
      "\u001b[0;32m<ipython-input-17-4437d268dc85>\u001b[0m in \u001b[0;36mfind_similar_words\u001b[0;34m(word, len_gap, N)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrigram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrigram_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0msimilar\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrigram_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrigram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msimilar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# TODO: replace with partial sort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     return sorted(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def check_corrector(queries: Queries, correct: Callable[[Query], Query], debug: bool):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(queries)\n",
    "    debug_output = 0\n",
    "    for original, fixed in progress:\n",
    "        if len(original.split(' ')) > 7:\n",
    "            continue\n",
    "        predict = correct(original)\n",
    "        if predict != fixed:\n",
    "            wrong += 1\n",
    "            if debug:\n",
    "                print(original)\n",
    "                print(fixed)\n",
    "                print(predict)\n",
    "                print()\n",
    "                debug_output += 1\n",
    "                if debug_output == 10:\n",
    "                    break\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')\n",
    "        \n",
    "check_corrector(queries, correct, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
