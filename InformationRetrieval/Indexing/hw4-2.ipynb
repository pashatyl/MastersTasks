{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Индексация\n",
    "Реализуйте построение инвертированного индекса в памяти для коллекции из домашней работы номер 3. В каждом постинглисте также сохраните значение term-frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "from typing import Dict, List, Tuple\n",
    "ArticleName = str\n",
    "Text = str\n",
    "Term = str\n",
    "TermCount = int\n",
    "DocLen = int\n",
    "CollectionData = None\n",
    "RankingParams = None\n",
    "RelevInfo = TermCount\n",
    "DocId = int\n",
    "TermFreq = int\n",
    "Posting = List[Tuple[DocId, RelevInfo]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15229it [00:00, 25724.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 15229/15229 [01:50<00:00, 138.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import parser1\n",
    "\n",
    "def load_docs(selected_docs_fn: ArticleName, threads: int = 4) -> Dict[ArticleName, Text]:    \n",
    "    docs = {}\n",
    "    pool = Pool(threads)\n",
    "    tasks = []\n",
    "    for line in tqdm(open(selected_docs_fn, encoding='utf8')):\n",
    "        article_name = line.strip()\n",
    "        tasks.append((article_name, pool.apply_async(parser1.get_article_text, (article_name, ))))\n",
    "    for article_name, task in tqdm(tasks):\n",
    "        docs[article_name] = task.get(10**6)\n",
    "    return docs\n",
    "    \n",
    "docs = load_docs(\"./selected_docs.tsv\", 32)\n",
    "fdocs = {k: v for k, v in docs.items() if len(v) > 0}\n",
    "docs, fdocs = fdocs, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from collections import defaultdict \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def make_terms(text: Text) -> List[Term]:\n",
    "    return [porter.stem(word) for word \n",
    "            in text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            if word not in stop_words]\n",
    "\n",
    "def invert_index(docs: Dict[ArticleName, Text]) -> Tuple[Dict[Term, Posting], Dict[ArticleName, DocId], float]:\n",
    "    inv_index = defaultdict(list)\n",
    "    article_to_doc_id = {}\n",
    "    doc_id_to_len = {}\n",
    "    terms_total = 0\n",
    "    for doc_id, (name, content) in enumerate(docs.items()):\n",
    "        article_to_doc_id[doc_id] = name\n",
    "        terms = make_terms(content)\n",
    "        doc_id_to_len[doc_id] = len(terms)\n",
    "        terms_total += len(terms)\n",
    "        for term, count in Counter(terms).items():\n",
    "            inv_index[term].append((doc_id, count))\n",
    "    return inv_index, article_to_doc_id, terms_total / len(docs), doc_id_to_len\n",
    "\n",
    "index, doc_id_to_article, mean_doc_len, doc_id_to_len = invert_index(docs)\n",
    "\n",
    "documents_terms = {k: make_terms(v) for k, v in docs.items()}\n",
    "term_index = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for name, content in documents_terms.items():\n",
    "    for term in content:\n",
    "        term_index[term][name] += 1\n",
    "        \n",
    "def tf(index, documents):\n",
    "    tfs = defaultdict(float)\n",
    "    for key, value in index.items():\n",
    "        for document, count in value.items():\n",
    "            tfs[(key, document)] = count * 1. / len(documents[document])\n",
    "            \n",
    "    return tfs\n",
    "\n",
    "def idf(index, documents):\n",
    "    return {term: math.log(len(documents) * 1. / len(value)) for term, value in index.items()}\n",
    "\n",
    "idfs = idf(term_index, documents_terms)\n",
    "tfs = tf(term_index, documents_terms)\n",
    "\n",
    "article_to_doc_id = {}\n",
    "for a, doc_id in doc_id_to_article.items():\n",
    "    article_to_doc_id[doc_id] = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраните полученный индекс на диске в бинарном формате. Формат должен позволять читать отсортированные по термам постинглисты, по одному за раз. Размер сохраненного индекса в байтах должен быть порядка 8*(сумму длин всех постинг листов). \n",
    "\n",
    "Отдельно сохраните на диск дополнительные данные о коллекции, которые пригодятся для поиска, например названия статей или среднюю длину документа. Размер дополнительных данных, должен быть пропорционален количеству документов коллекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index file size: 15359637\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def dump_index(index: Dict[Term, Posting], filename: str) -> None:\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(struct.pack(\"i\", len(index)))\n",
    "        for term, posting in index.items():\n",
    "            bytes_term = term.encode()\n",
    "            bytes_term_len = len(bytes_term)\n",
    "            p_len = len(posting)\n",
    "            assert bytes_term_len < 2**32 - 1 and p_len < 2**32 - 1\n",
    "            doc_ids = [p[0] for p in posting]\n",
    "            freqs = [p[1][0] for p in posting]\n",
    "\n",
    "            f.write(struct.pack(\"i\", bytes_term_len))\n",
    "            f.write(struct.pack(f\"{len(bytes_term)}s\", bytes_term))\n",
    "            f.write(struct.pack(\"i\", p_len))\n",
    "            f.write(struct.pack(f\"{p_len}i\", *doc_ids))\n",
    "            f.write(struct.pack(f\"{p_len}i\", *freqs))\n",
    "\n",
    "\n",
    "# def dump_collectiondata(data: CollectionData, filename: str) -> None:\n",
    "#     #\n",
    "#     #\n",
    "#     #\n",
    "#     # Code here\n",
    "#     #\n",
    "#     #\n",
    "#     #\n",
    "#     #\n",
    "\n",
    "dump_index(index, \"index.inv\")\n",
    "print(\"Index file size:\", os.path.getsize(\"index.inv\"))\n",
    "\n",
    "# dump_collectiondata(collection_data, \"index.data\")\n",
    "# print(\"Collection data file size:\", os.path.getsize(\"index.data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Поиск\n",
    "Для простоты реализации поиска, не требуется делать чтение постинглистов с диска по запросу - достаточно считать их с диска в память целиком. Также загрузите с диска дополнительные данные о коллекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number or terms in index: 186165\n"
     ]
    }
   ],
   "source": [
    "def load_index(filename: str) -> Dict[Term, Posting]:\n",
    "    index = {}\n",
    "    with open(filename, \"rb\") as f:\n",
    "        size = struct.unpack(\"i\", f.read(4))[0]\n",
    "        for _ in range(size):\n",
    "            term_len = struct.unpack(\"i\", f.read(4))[0]\n",
    "            term_b = f.read(term_len)\n",
    "            term = struct.unpack(f\"{term_len}s\", term_b)[0].decode()\n",
    "\n",
    "            bytes_arr_len = struct.unpack(\"i\", f.read(4))[0]\n",
    "            doc_ids_b = f.read(bytes_arr_len * 4)\n",
    "            tfs_b = f.read(bytes_arr_len * 4)\n",
    "            doc_ids = struct.unpack(f\"{bytes_arr_len}i\", doc_ids_b)\n",
    "            tfs = struct.unpack(f\"{bytes_arr_len}i\", tfs_b)\n",
    "            index[term] = list(zip(doc_ids, tfs))\n",
    "\n",
    "    return index   \n",
    "\n",
    "    \n",
    "# def load_collectiondata(filename: str) -> CollectionData:\n",
    "#     #\n",
    "#     #\n",
    "#     #\n",
    "#     # Code here\n",
    "#     #\n",
    "#     #\n",
    "#     #\n",
    "#     #\n",
    "\n",
    "index = load_index(\"index.inv\")\n",
    "print(\"Number or terms in index:\", len(index))\n",
    "# collection_data = load_collectiondata(\"index.data\")\n",
    "collection_data = (doc_id_to_article, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте поиск документовна с ранжированием BM25 на основе инвертированного индекса в парадигме document-at-time, то есть через [слияние](https://en.wikipedia.org/wiki/Merge_algorithm) постинглистов. Функция поиска должна принимать число - ограничение на количество документов, возвращаемое поиском. Используемое количество дополнительной памяти должно быть пропорционально этому ограничению и никак не должно зависить от размера постинглистов или размера коллекции.\n",
    "Результаты поиска должны быть аналогичные тем, что были в домашней работе номер 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heapify, heappush, heappushpop, heappop\n",
    "\n",
    "class MinHeap:\n",
    "    def __init__(self, top_n: int):\n",
    "        self.h = []\n",
    "        self.length = top_n\n",
    "        heapify(self.h)\n",
    "\n",
    "    def add(self, element: int):\n",
    "        if len(self.h) < self.length:\n",
    "            heappush(self.h, element)\n",
    "        else:\n",
    "            heappushpop(self.h, element)\n",
    "    \n",
    "    def top(self):\n",
    "        return self.h[0]\n",
    "    \n",
    "    def values(self):\n",
    "        return self.h\n",
    "    \n",
    "    def pop(self):\n",
    "        return heappop(self.h)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.h)\n",
    "    \n",
    "def test_search(search):\n",
    "    for query in \n",
    "            [\n",
    "                \"coronovirus in belarus\",\n",
    "                \"who won junior eurovision in 2005\",\n",
    "                \"science about full-text search\",\n",
    "            ]:\n",
    "        result = search(query)[:5]\n",
    "        print(f\"[{query}]\")\n",
    "        for article_name, score in result:\n",
    "            print(f\"{score:7.2f}  {article_name}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[coronovirus in belarus]\n",
      "  10.13  Time_in_Belarus\n",
      "   9.80  COVID-19_pandemic_in_Belarus\n",
      "   9.75  Daugava_River\n",
      "   9.72  Bug_River\n",
      "   9.38  Eurasian_Union\n",
      "\n",
      "\n",
      "[who won junior eurovision in 2005]\n",
      "  20.65  Junior_Eurovision_Song_Contest_2019\n",
      "  19.01  Junior_Eurovision_Song_Contest_2015\n",
      "  18.61  Junior_Eurovision_Song_Contest_2004\n",
      "  18.54  Junior_Eurovision_Song_Contest_2014\n",
      "  15.16  Blue_(group)\n",
      "\n",
      "\n",
      "[science about full-text search]\n",
      "  23.33  Information_retrieval\n",
      "  12.48  On-Line_Encyclopedia_of_Integer_Sequences\n",
      "  11.79  Scientist\n",
      "  10.98  Citizen_science\n",
      "  10.69  Binary_search\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "from collections import deque\n",
    "\n",
    "def search_indexed(query: Text, top_size: int, index: Dict[Term, Posting], collection_data: CollectionData, b, k1, k2, type) -> List[Tuple[ArticleName, float]]:\n",
    "    query_terms = Counter(filter(lambda x: x in index, make_terms(query)))\n",
    "    postings = [deque(copy(index[t])) for t in query_terms.keys()]\n",
    "    posting_heap = MinHeap(len(query_terms))\n",
    "    for i, (term, posting) in enumerate(zip(query_terms.keys(), postings)):\n",
    "        posting_heap.add((posting[0], i))\n",
    "    \n",
    "    if len(query_terms) == 0:\n",
    "        return [('', 0) for _ in range(top_size)]\n",
    "    \n",
    "    top = MinHeap(top_size)\n",
    "    prev_doc_id = posting_heap.top()[0][0]\n",
    "    bm25 = 0\n",
    "    while len(posting_heap) > 0:\n",
    "        (doc_id, tf), term_id = posting_heap.top()\n",
    "        posting_heap.pop()\n",
    "        postings[term_id].popleft()\n",
    "        if len(postings[term_id]) != 0:\n",
    "            posting_heap.add((postings[term_id][0], term_id))\n",
    "#             print(postings[term_id][0])\n",
    "        if doc_id != prev_doc_id:\n",
    "            top.add((bm25, prev_doc_id))\n",
    "            bm25 = 0\n",
    "            \n",
    "        K = k1 * ((1 - b) + b * doc_id_to_len[doc_id] * 1. / mean_doc_len)\n",
    "        term = list(query_terms.keys())[term_id]\n",
    "        bm25 += idfs[term] * (k1 + 1) * tf * 1. / (K + tf) * \\\n",
    "                (k2 + 1) * query_terms[term] * 1. / (k2 + query_terms[term])\n",
    "        prev_doc_id = doc_id\n",
    "    \n",
    "    top.add((bm25, prev_doc_id))\n",
    "\n",
    "    return sorted([(collection_data[0][doc_id], bm25) for bm25, doc_id in top.values()], key = lambda x: -x[1])\n",
    "\n",
    "ranking_params = {'type': '', \"b\" : 1, \"k1\" : 1, \"k2\" : 1}\n",
    "test_search(lambda x: search_indexed(x, 5, dict(index), collection_data, **ranking_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statistics import mean \n",
    "\n",
    "# def search_bm25(query, b, k1, k2):\n",
    "# #     query_terms = defaultdict(int)\n",
    "# #     query = make_terms(query)\n",
    "\n",
    "#     query_terms = Counter(filter(lambda x: x in index, make_terms(query)))\n",
    "\n",
    "#     documents_range = []\n",
    "#     for name, content in documents_terms.items():\n",
    "# #         print(content)\n",
    "#         if len(content) == 0:\n",
    "#             continue\n",
    "        \n",
    "#         K = k1 * ((1 - b) + b * (len(content) * 1. / mean_doc_len))\n",
    "#         result = 0.\n",
    "#         for term in query_terms.keys():\n",
    "#             tf = tfs[(term, name)]\n",
    "#             result += idfs[term] * (k1 + 1) * tf * 1. / (K + tf) * \\\n",
    "#                 (k2 + 1) * query_terms[term] * 1. / (k2 + query_terms[term])\n",
    "#         documents_range.append((name, result))\n",
    "        \n",
    "#     return sorted(documents_range, key=lambda x: -x[1])\n",
    "\n",
    "# test_search(lambda x: search_bm25(x, **ranking_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте static pruning до 50 элементов для каждого постинглиста. Сравните качество и скорость работы нового индекса с предыдущим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(index: Dict[Term, Posting], top_size: int = 50) -> Dict[Term, Posting]:\n",
    "    return {term: sorted(sorted(posting, key=lambda x: -x[1])[:top_size], key=lambda x: x[0]) \n",
    "            for term, posting in index.items()}\n",
    "\n",
    "pruned_index = prune(index, 50)\n",
    "for term, posting in pruned_index.items():\n",
    "    prev_doc_id = -1\n",
    "    for doc_id, freq in posting:\n",
    "        assert doc_id > prev_doc_id\n",
    "        prev_doc_id = doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[coronovirus in belarus]\n",
      "  19.39  Time_in_Belarus\n",
      "  18.70  COVID-19_pandemic_in_Belarus\n",
      "  16.55  Byelorussian_Soviet_Socialist_Republic\n",
      "  16.31  Eurasian_Union\n",
      "  15.42  Daugava_River\n",
      "\n",
      "\n",
      "[who won junior eurovision in 2005]\n",
      "  31.93  Junior_Eurovision_Song_Contest_2019\n",
      "  26.18  Junior_Eurovision_Song_Contest_2004\n",
      "  25.15  Junior_Eurovision_Song_Contest_2015\n",
      "  24.32  Junior_Eurovision_Song_Contest_2014\n",
      "  23.20  Eurovision:_Europe_Shine_a_Light\n",
      "\n",
      "\n",
      "[science about full-text search]\n",
      "  23.75  Information_retrieval\n",
      "  18.27  Google_Search\n",
      "  15.83  Binary_search\n",
      "  14.38  Ask.com\n",
      "  13.94  Aliweb\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_search(lambda x: search_indexed(x, 5, pruned_index, collection_data, **ranking_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните качество и скорость работы нового алгоритма поиска с предыдущим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc: 0.28, Acc10: 0.56, RR: 0.36: 100%|██████████████████████████████████████████████| 199/199 [00:04<00:00, 45.06it/s]\n",
      "Acc: 0.24, Acc10: 0.48, RR: 0.32:  25%|███████████▌                                  | 50/199 [00:00<00:00, 499.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index merge\n",
      "  Accuracy: 0.28\n",
      "  Accuracy10: 0.56\n",
      "  RR: 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc: 0.25, Acc10: 0.46, RR: 0.33: 100%|█████████████████████████████████████████████| 199/199 [00:00<00:00, 479.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 pruned\n",
      "  Accuracy: 0.25\n",
      "  Accuracy10: 0.46\n",
      "  RR: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_queries(queries_fn: ArticleName) -> List[Tuple[Text, ArticleName]]:\n",
    "    queries = []\n",
    "    for line in open(queries_fn):\n",
    "        query, answer = line.rstrip().split('\\t', 1)\n",
    "        queries.append((query, answer))\n",
    "    return queries\n",
    "\n",
    "queries = list(filter(lambda x: x[1] in docs, load_queries(\"./queries.tsv\")))\n",
    "for query, answer in queries:\n",
    "    assert answer in docs\n",
    "\n",
    "def search(query, ranking_params = None):\n",
    "    if ranking_params[\"type\"] == \"bm25\":\n",
    "        return search_indexed(query, 10, index, collection_data, **ranking_params)\n",
    "    elif ranking_params[\"type\"] == \"bm25_pruned\":\n",
    "        return search_indexed(query, 10, pruned_index, collection_data, **ranking_params)\n",
    "\n",
    "def run(\n",
    "    title,\n",
    "    queries: List[Tuple[Text, ArticleName]],\n",
    "    collection_data: CollectionData,\n",
    "    ranking_params: RankingParams) -> None:\n",
    "    accuracy = 0.0\n",
    "    accuracy10 = 0.0\n",
    "    rr = 0.0\n",
    "    processed = 0\n",
    "    with tqdm(queries) as progress:\n",
    "        for query, answer in progress:\n",
    "            result = search(query, ranking_params)[:10]\n",
    "            \n",
    "            rank = None\n",
    "            for position, (article_name, score) in enumerate(result):\n",
    "                if article_name == answer:\n",
    "                    rank = position + 1\n",
    "                    break\n",
    "                \n",
    "            if rank is not None:\n",
    "                accuracy += (rank == 1)\n",
    "                accuracy10 += (rank <= 10)\n",
    "                rr += 1.0 / rank\n",
    "                \n",
    "            processed += 1\n",
    "            progress.set_description(f'Acc: {accuracy/processed:0.2f}, Acc10: {accuracy10/processed:0.2f}, RR: {rr/processed:0.2f}')\n",
    "    print(f'{title}\\n  Accuracy: {accuracy/processed:0.2f}\\n  Accuracy10: {accuracy10/processed:0.2f}\\n  RR: {rr/processed:0.2f}')\n",
    "\n",
    "ranking_params = {'type': 'bm25', 'b': 0.625, 'k1': 4, 'k2': 1} \n",
    "run(\"BM25 index merge\", queries, documents_terms, ranking_params)\n",
    "\n",
    "ranking_params['type'] = \"bm25_pruned\"\n",
    "run(\"BM25 pruned\", queries, documents_terms, ranking_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из lab3:  \n",
    "BM25{'type': 'bm25', 'b': 0.625, 'k1': 4, 'k2': 1}  \n",
    "Accuracy: 0.28  \n",
    "Accuracy10: 0.57  \n",
    "RR: 0.36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~Дополнительно~\n",
    "### Сжатие индекса (+1 балл)\n",
    "Реализуйте кодирование чисел алгоритмом VarInt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x02\\x03\\xac\\x02\\xa0\\x9c\\x01'\n",
      "[1, 2, 3, 300, 20000]\n"
     ]
    }
   ],
   "source": [
    "import struct \n",
    "import io\n",
    "\n",
    "\n",
    "class baseline_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        for doc_id, freq in posting:\n",
    "            output_stream.write(struct.pack('II', doc_id, freq))\n",
    "            \n",
    "    def decode(input_stream):\n",
    "        res = []\n",
    "        while True:\n",
    "            data = input_stream.read(struct.calcsize('II'))\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            res.append(struct.unpack('II', data))\n",
    "        return res\n",
    "\n",
    "\n",
    "class varint_coder:\n",
    "    def encode_num(number):\n",
    "        buf = b''\n",
    "        while True:\n",
    "            towrite = number & 0x7f\n",
    "            number >>= 7\n",
    "            if number != 0:\n",
    "                buf += (towrite | 0x80).to_bytes(1, byteorder='big')\n",
    "            else:\n",
    "                buf += (towrite).to_bytes(1, byteorder='big')\n",
    "                break\n",
    "        return buf\n",
    "    \n",
    "    def decode_num(input_stream):\n",
    "        shift = 0\n",
    "        result = 0\n",
    "        i = 0x80\n",
    "        while i & 0x80 != 0:\n",
    "            i = ord(input_stream.read(1))\n",
    "            result |= (i & 0x7f) << shift\n",
    "            shift += 7\n",
    "\n",
    "        return shift / 7, result\n",
    "    \n",
    "    def decode(input_stream):\n",
    "        size = len(input_stream.getvalue())\n",
    "        total_read = 0\n",
    "        res = []\n",
    "        while total_read < size:\n",
    "            read, data = varint_coder.decode_num(input_stream)\n",
    "            total_read += read\n",
    "            res.append(data)\n",
    "        return res\n",
    "        \n",
    "    def encode(output_stream, posting):\n",
    "        for num in posting:\n",
    "            output_stream.write(varint_coder.encode_num(num))\n",
    "\n",
    "output = io.BytesIO()\n",
    "varint_coder.encode(output, [1, 2, 3, 300, 20000])\n",
    "print(output.getvalue())\n",
    "\n",
    "posting = varint_coder.decode(io.BytesIO(output.getvalue()))\n",
    "print(posting)\n",
    "assert posting == [1, 2, 3, 300, 20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните эффективность разных вариантов кодирования постинглистов:\n",
    " - Базовый вариант (4 байта на число)\n",
    " - Какой-нибудь алгоритм сжатия общего назначения (lz4/zstd/brotli/gzip)\n",
    " - VarInt\n",
    " - Delta-кодирование + Какой-нибудь алгоритм сжатия общего назначения \n",
    " - Delta-кодирование + VarInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 186165/186165 [00:01<00:00, 115628.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_coder: 11.728889465332031 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_encoded_size(coder, index):\n",
    "    total_size = 0\n",
    "    for term, posting in tqdm(index.items()):\n",
    "        if len(posting) == 0:\n",
    "            continue\n",
    "        output = io.BytesIO()\n",
    "        coder.encode(output, posting)\n",
    "        data = output.getvalue()\n",
    "        total_size += len(data)\n",
    "        decoded_posting = coder.decode(io.BytesIO(data))\n",
    "        assert decoded_posting == posting, f\"{decoded_posting} != {posting}\"\n",
    "    print(f\"{coder.__name__}: {total_size/1024/1024} MB\")    \n",
    "    \n",
    "test_encoded_size(baseline_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 186165/186165 [00:06<00:00, 28236.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varint_posting_coder: 4.381299018859863 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class varint_posting_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        unzipped_posting = list(zip(*posting))\n",
    "        varint_coder.encode(output_stream, unzipped_posting[0])\n",
    "        varint_coder.encode(output_stream, unzipped_posting[1])\n",
    "            \n",
    "    def decode(input_stream):\n",
    "        decoded = varint_coder.decode(input_stream)\n",
    "        left = decoded[:len(decoded)//2]\n",
    "        right = decoded[len(decoded)//2:]\n",
    "        return list(zip(left, right))\n",
    "    \n",
    "test_encoded_size(varint_posting_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 186165/186165 [00:02<00:00, 71988.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lz4_coder: 11.713878631591797 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import lz4.frame\n",
    "\n",
    "class lz4_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        output_stream0 = io.BytesIO()\n",
    "        baseline_coder.encode(output_stream0, posting)\n",
    "        output_stream.write(lz4.frame.compress(output_stream0.getvalue()))\n",
    "            \n",
    "    def decode(input_stream):\n",
    "        decompressed = lz4.frame.decompress(input_stream.getvalue())\n",
    "        return baseline_coder.decode(io.BytesIO(decompressed))\n",
    "    \n",
    "test_encoded_size(lz4_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 186165/186165 [00:03<00:00, 54354.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_coder: 11.745101928710938 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def delta_encode(posting):\n",
    "    delta_posting = []\n",
    "    last = -1\n",
    "    for doc_id, f in posting:\n",
    "        if last == -1:\n",
    "            delta_posting.append((doc_id, f))\n",
    "        else:\n",
    "            delta_posting.append((doc_id - last - 1, f))\n",
    "        last = doc_id\n",
    "    return delta_posting\n",
    "\n",
    "def delta_decode(delta_posting):\n",
    "    posting = []\n",
    "    last = -1\n",
    "    for doc_id, f in delta_posting:\n",
    "        if last == -1:\n",
    "            last = doc_id\n",
    "        else:\n",
    "            last = doc_id + last + 1\n",
    "        posting.append((last, f))\n",
    "    return posting\n",
    "\n",
    "class delta_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        delta_posting = delta_encode(posting)\n",
    "        lz4_coder.encode(output_stream, delta_posting)\n",
    "        \n",
    "    def decode(input_stream):\n",
    "        delta_posting = lz4_coder.decode(input_stream)\n",
    "        return delta_decode(delta_posting)\n",
    "\n",
    "test_encoded_size(delta_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 186165/186165 [00:05<00:00, 32483.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_varint_coder: 3.4431543350219727 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class delta_varint_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        delta_posting = delta_encode(posting)\n",
    "        varint_posting_coder.encode(output_stream, delta_posting)\n",
    "        \n",
    "    def decode(input_stream):\n",
    "        delta_posting = varint_posting_coder.decode(input_stream)\n",
    "        return delta_decode(delta_posting)\n",
    "    \n",
    "test_encoded_size(delta_varint_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
